# Глава 2. Разработка моделей машинного обучения

## 2.2 Сбор и предобработка датасета

Процесс подготовки данных для моделирования является критически важным этапом, обеспечивающим качество и надежность будущих моделей машинного обучения. Этот этап включал сбор исходных данных, их очистку, трансформацию и конструирование новых признаков, направленных на наиболее полное отражение лояльности клиентов.

### 2.2.1 Сбор и первичный анализ данных

Исходные данные были предоставлены компанией Acoola в виде единого CSV-файла (`Concept202408.csv`) размером около 113 МБ. Первичная загрузка показала, что датасет содержит 418 888 записей и 19 исходных признаков, описывающих транзакции клиентов, их участие в программе лояльности и основную клиентскую информацию.

### 2.2.2 Очистка данных

На этапе очистки данных были выполнены следующие процедуры:

*   **Обработка пропущенных значений:** В исходном датасете было обнаружено 7679 пропущенных значений. Для их обработки использовались методы заполнения, специфичные для каждого признака (например, заполнение медианой или модой для числовых и категориальных признаков соответственно, или использование специфических значений, таких как 0 для бонусов, если это логично по смыслу признака). После обработки все пропущенные значения были устранены.
*   **Обработка выбросов:** Для числовых признаков, таких как 'Cумма покупки', 'Начислено бонусов', 'Списано бонусов', 'Средняя сумма покупок', 'Частота, раз/мес', 'Баланс накопленный' и 'Баланс подарочный', был проведен анализ на наличие выбросов. Использовался метод межквартильного размаха (IQR), и значения, выходящие за пределы `Q1 - 1.5*IQR` и `Q3 + 1.5*IQR`, идентифицировались как выбросы. Например, для признака 'Cумма покупки' было обнаружено 2.81% выбросов, а для 'Списано бонусов' – 19.10%. Выбросы обрабатывались методом винсоризации (замена экстремальных значений на ближайшие значения в пределах установленных границ) для снижения их влияния на последующий анализ и моделирование.
*   **Проверка и коррекция типов данных:** Была проведена проверка типов данных всех столбцов. Даты были преобразованы в соответствующий формат datetime, числовые признаки – в float или int, категориальные – в object/string.

### 2.2.3 Трансформация данных

После очистки данные были трансформированы для подготовки к этапу моделирования:

*   **Кодирование категориальных переменных:**
    *   Для бинарного признака 'Пол' применялось целочисленное кодирование (Label Encoding), где, например, 'M' отображалось в 0, а 'Ж' – в 1. Соответствующий `LabelEncoder` и маппинг были сохранены.
    *   Для признака 'Точка продаж' (локации) был применен метод One-Hot Encoding. Для снижения размерности были выбраны топ-10 наиболее частых локаций, а остальные объединены в категорию 'Other'.
    *   Аналогично, для агрегированного признака 'Категория товара' (`product_category`) также использовался One-Hot Encoding для выделения отдельных товарных групп (например, 'Аксессуары', 'Брюки', 'Верхняя одежда' и т.д.).
*   **Нормализация и стандартизация числовых признаков:**
    *   К денежным показателям ('Cумма покупки', 'Начислено бонусов', 'Списано бонусов', 'Средняя сумма покупок', 'Баланс накопленный', 'Баланс подарочный') было применено масштабирование к диапазону [0, 1] с использованием `MinMaxScaler`.
    *   Для частотных переменных ('Частота, раз/мес', 'Покупок, в днях') применялась стандартизация (приведение к нулевому среднему и единичному стандартному отклонению) с использованием `StandardScaler`.
    Все использованные трансформеры (`MinMaxScaler`, `StandardScaler`, `LabelEncoder`) были сохранены для последующего применения на новых данных (например, в `output/models/transformers`).

### 2.2.4 Конструирование признаков (Feature Engineering)

Для обогащения датасета и более точного представления факторов лояльности были сконструированы новые признаки:

*   **Агрегация данных на уровне клиента:** Исходные транзакционные данные были агрегированы на уровень уникального клиента. Для числовых признаков (например, 'Cумма покупки') рассчитывались различные агрегаты (количество, сумма, среднее, стандартное отклонение). Для категориальных признаков (например, One-Hot encoded локации или категории товаров) рассчитывалась средняя доля или первое встреченное значение. Это позволило получить профиль каждого клиента с 37 признаками после агрегации (до дальнейшего инжиниринга).
*   **RFM-анализ:** Был проведен классический RFM-анализ, включающий расчет давности последней покупки (Recency), частоты покупок (Frequency) и общей суммы покупок (Monetary). На основе этих метрик клиенты были сегментированы на несколько категорий лояльности (например, "Высоколояльные", "Лояльные", "Отток" и т.д.). Результаты базового RFM-анализа и сегментации сохранялись (например, в `output/base_rfm_dataset.pkl`).
*   **Разработка расширенных признаков лояльности:**
    *   **Признаки использования бонусной программы:** Были созданы признаки, отражающие активность использования бонусов, такие как `bonus_usage_ratio` (отношение списанных бонусов к начисленным), `bonus_earning_ratio` (отношение начисленных бонусов к сумме покупок), `bonus_activity` (общая сумма операций с бонусами).
    *   **Признаки стабильности покупок:** Рассчитывались метрики, характеризующие регулярность и стабильность покупательского поведения, например, `purchase_density` (плотность покупок во времени), `purchase_amount_cv` (коэффициент вариации суммы покупки), `purchase_stability`.
    *   **Взвешенный RFM-показатель:** Был рассчитан взвешенный RFM-показатель, где компонентам R, F, M присваивались различные веса (например, R=0.5, F=0.3, M=0.2), отражающие их предполагаемую важность для оценки лояльности.
    *   Всего было создано около 7 новых расширенных признаков лояльности.
*   **Кластеризация клиентов:** Для выявления естественных групп клиентов со схожим поведением применялась кластеризация методом K-Means. В качестве признаков для кластеризации использовался набор из 40 числовых переменных, включая RFM-метрики, агрегированные показатели покупок, использования бонусов и другие поведенческие характеристики. Данные предварительно стандартизировались. Было выделено 5 кластеров. Модель `KMeans` и использованный `StandardScaler`, а также список признаков для кластеризации были сохранены.
*   **Применение метода главных компонент (PCA):** Для снижения размерности и выделения обобщенных факторов поведения клиентов использовался PCA. PCA применялся к тому же набору из 40 признаков, что и для кластеризации (после стандартизации). Было решено использовать 3 главные компоненты. Модель `PCA` и соответствующий `StandardScaler` были сохранены.
*   **Формирование итогового целевого признака лояльности:** На основе взвешенного RFM, бонусных признаков, кластера клиента и PCA-компонент был рассчитан единый `enhanced_loyalty_score`. Этот непрерывный показатель затем был преобразован в дискретные категории лояльности (например, 'Высоколояльные', 'Отток') с использованием квантильного метода. В данном конкретном запуске (`log.txt`) получилось две категории: 'Высоколояльные' (87.37%) и 'Отток' (12.63%), что указывало на значительный дисбаланс классов. Была проведена оценка распределения клиентов по категориям и отмечена необходимость возможной оптимизации границ или применения методов балансировки. Целевая переменная `loyalty_target` была создана путем кодирования этих категорий (например, 'Отток' -> 0, 'Высоколояльные' -> 1).

### 2.2.5 Подготовка итогового датасета для моделирования

Финальный этап подготовки данных включал:

*   **Балансировка классов:** Из-за выявленного дисбаланса в целевой переменной `loyalty_target` была предусмотрена возможность применения различных методов балансировки классов (например, SMOTE, oversampling, undersampling). В рассматриваемом запуске (`log.txt`) балансировка была отключена (`method='none'`).
*   **Оценка информативности и отбор признаков:** Была проведена оценка важности всех доступных признаков (44 признака на данном этапе) с использованием нескольких методов: корреляционного анализа (Пирсон), взаимной информации, Permutation Importance и SHAP values (для моделей, поддерживающих этот метод). На основе интегральной оценки важности был произведен отбор наиболее информативных признаков. В данном запуске было отобрано 20 признаков с порогом важности 0.05. Список топ-10 признаков включал `recency_ratio`, `monetary`, `pca_component_3`, `recency` и др. Результаты оценки важности сохранялись (например, в `output/feature_importance/feature_importance.csv`).
*   **Разделение на обучающую и тестовую выборки:** Итоговый датасет с отобранными признаками был разделен на обучающую (80%) и тестовую (20%) выборки. Разделение производилось стратифицированно по целевой переменной `loyalty_target` для сохранения исходного распределения классов в обеих выборках.
*   **Сохранение результатов:** Все промежуточные и итоговые датасеты, а также метаданные (например, списки отобранных признаков, параметры моделей предобработки) сохранялись для воспроизводимости и дальнейшего использования (например, `output/preprocessed_data.csv`, `output/loyalty_features_dataset.pkl`, `output/loyalty_dataset.pkl`, данные для обучения/тестирования в `output/model_data`).

В результате всех этих шагов был получен очищенный, преобразованный и обогащенный набор данных, полностью готовый для обучения и оценки моделей классификации клиентов по уровню лояльности. Итоговый датасет для моделирования содержал 20 признаков. 